{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homework(train_X, valid_X, train_Y, valid_Y, tokenizer_iu):\n",
    "    import numpy as np\n",
    "    import copy\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input, Embedding, Dense, LSTM, Flatten\n",
    "    from keras.layers import Permute, Activation, concatenate, dot, Reshape\n",
    "    from keras import backend as K\n",
    "\n",
    "    emb_dim = 256\n",
    "    hid_dim = 256\n",
    "    att_dim = 256\n",
    "\n",
    "    vocab_size = len(tokenizer_iu.word_index) + 1\n",
    "    seq_len = len(train_Y[0])\n",
    "\n",
    "    en_in = Input(shape=(9, 9, 512))\n",
    "    en_f = Flatten()(en_in)\n",
    "    # decorderのLSTMへの入力に変換（先に、再利用のための「層の変数化」が必要）\n",
    "    dense_h = Dense(hid_dim)\n",
    "    dense_c = Dense(hid_dim)\n",
    "    h_0 = dense_h(en_f)\n",
    "    c_0 = dense_c(en_f)\n",
    "    # Attention利用時にそなえてreshape: VGGなら(9*9, 512)へ\n",
    "    en_map = Reshape((81, 512))(en_f)\n",
    "\n",
    "    # decorderのインプット\n",
    "    de_in = Input(shape=(seq_len,))\n",
    "\n",
    "# 層の定義（再利用用）\n",
    "    de_embedding = Embedding(vocab_size, emb_dim)\n",
    "    de_lstm = LSTM(hid_dim, activation='tanh', return_sequences=True, return_state=True)\n",
    "# 接続\n",
    "    de_emb = de_embedding(de_in)\n",
    "    de_out, _, _ = de_lstm(de_emb, initial_state=[h_0, c_0])\n",
    "\n",
    "    ## Attension\n",
    "    # 1. スコアの計算 （ドット積）\n",
    "    sc_dense = Dense(hid_dim)\n",
    "    en_score = sc_dense(en_map)\n",
    "    score = dot([de_out, en_score], axes=(2,2))\n",
    "    # 2. 重み計算 (Attention, softmax関数)\n",
    "    attention = Activation('softmax')(score) \n",
    "    # 3. 文脈ベクトルの計算（エンコーダの出力に重みをかける）\n",
    "    context = dot([attention, en_map], axes=(2,1)) \n",
    "    # 4. 出力ベクトルの計算（文脈とデコーダー出力を結合→Dense層へ）\n",
    "    out_dens1 = Dense(att_dim, activation='tanh')\n",
    "    out_dens2 = Dense(vocab_size, activation='softmax')\n",
    "    concat = concatenate([context, de_out], axis=2)\n",
    "    at_out = out_dens1(concat)\n",
    "    output = out_dens2(at_out)\n",
    "\n",
    "    # モデル構築（入力は符号化器＆復号化器、出力は復号化器のみ）\n",
    "    model = Model([en_in, de_in], output)\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    # 学習（Lesson4のを利用、validを別に与える）\n",
    "    train_target = np.hstack((train_Y[:, 1:], np.zeros((len(train_Y),1), dtype=np.int32)))\n",
    "    valid_target = np.hstack((valid_Y[:, 1:], np.zeros((len(valid_Y),1), dtype=np.int32)))\n",
    "    model.fit([train_X, train_Y], np.expand_dims(train_target, -1), batch_size=128, epochs=80, verbose=2, validation_data=([valid_X, valid_Y], np.expand_dims(valid_target, -1)))\n",
    "\n",
    "    # 予測用。Encoder側。Dence層のみ。\n",
    "    en_in = Input(shape=(9, 9, 512))\n",
    "    en_f = Flatten()(en_in)\n",
    "    en_map = Reshape((81, 512))(en_f)\n",
    "    h_0 = dense_h(en_f)\n",
    "    c_0 = dense_c(en_f)\n",
    "    en_model = Model([en_in], [en_map, h_0, c_0])\n",
    "\n",
    "    # Decoder側。Lesson 4はAttentionと別だが１つにまとめる。\n",
    "    h_tm1 = Input(shape=(hid_dim,))\n",
    "    c_tm1 = Input(shape=(hid_dim,))\n",
    "    de_in = Input(shape=(1,))\n",
    "    en_map_in = Input(shape=(81, 512,))\n",
    "    # DecoderのLSTM\n",
    "    de_emb = de_embedding(de_in) # 学習済みEmbeddingレイヤーを利用\n",
    "    de_out, de_state1, de_state2 = de_lstm(de_emb, initial_state=[h_tm1, c_tm1]) # 学習済みLSTMレイヤーを利用\n",
    "    ## Attension\n",
    "    en_score = sc_dense(en_map_in)\n",
    "    score = dot([de_out, en_score], axes=(2,2))\n",
    "    attention = Activation('softmax')(score) \n",
    "    context = dot([attention, en_map_in], axes=(2,1)) \n",
    "    concat = concatenate([context, de_out], axis=2)\n",
    "    at_out = out_dens1(concat)\n",
    "    output = out_dens2(at_out)\n",
    "\n",
    "    de_model = Model([de_in, h_tm1, c_tm1, en_map_in], [output, de_state1, de_state2])\n",
    "\n",
    "    # 出力をIUPAC名に戻す\n",
    "    def decode_sequence(input_seq, bos_eos, max_output_length = 100):\n",
    "        input_seq = np.reshape(input_seq, (1, 9, 9, 512))\n",
    "        map, state1, state2 = en_model.predict(input_seq)\n",
    "\n",
    "        target_seq = np.array(bos_eos[0])\n",
    "        output_seq= copy.deepcopy(bos_eos[0]) # ここが重要！\n",
    "    \n",
    "        while True:\n",
    "            output_tokens, state1, state2 = de_model.predict([target_seq, state1, state2, map])\n",
    "            sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "            output_seq += sampled_token_index\n",
    "        \n",
    "            if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "                break\n",
    "\n",
    "            target_seq = np.array(sampled_token_index)\n",
    "\n",
    "        return output_seq\n",
    "\n",
    "    def save_model(model, name):\n",
    "        data_dir = '/root/userspace/data/data/'\n",
    "        if not os.path.isdir(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        result_dir = os.path.normpath(data_dir)\n",
    "#        model.save_weights(os.path.join(result_dir, name + '_model.h5'))\n",
    "\n",
    "    return decode_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21480 samples, validate on 283 samples\n",
      "Epoch 1/80\n",
      " - 35s - loss: 0.9940 - val_loss: 0.6479\n",
      "Epoch 2/80\n",
      " - 34s - loss: 0.5381 - val_loss: 0.4876\n",
      "Epoch 3/80\n",
      " - 34s - loss: 0.4203 - val_loss: 0.4125\n",
      "Epoch 4/80\n",
      " - 34s - loss: 0.3501 - val_loss: 0.3703\n",
      "Epoch 5/80\n",
      " - 34s - loss: 0.3035 - val_loss: 0.3375\n",
      "Epoch 6/80\n",
      " - 34s - loss: 0.2692 - val_loss: 0.3243\n",
      "Epoch 7/80\n",
      " - 34s - loss: 0.2421 - val_loss: 0.3041\n",
      "Epoch 8/80\n",
      " - 34s - loss: 0.2191 - val_loss: 0.2918\n",
      "Epoch 9/80\n",
      " - 34s - loss: 0.2021 - val_loss: 0.2774\n",
      "Epoch 10/80\n",
      " - 34s - loss: 0.1833 - val_loss: 0.2713\n",
      "Epoch 11/80\n",
      " - 34s - loss: 0.1681 - val_loss: 0.2734\n",
      "Epoch 12/80\n",
      " - 34s - loss: 0.1542 - val_loss: 0.2611\n",
      "Epoch 13/80\n",
      " - 34s - loss: 0.1432 - val_loss: 0.2626\n",
      "Epoch 14/80\n",
      " - 34s - loss: 0.1356 - val_loss: 0.2481\n",
      "Epoch 15/80\n",
      " - 34s - loss: 0.1270 - val_loss: 0.2500\n",
      "Epoch 16/80\n",
      " - 34s - loss: 0.1210 - val_loss: 0.2489\n",
      "Epoch 17/80\n",
      " - 34s - loss: 0.1137 - val_loss: 0.2550\n",
      "Epoch 18/80\n",
      " - 34s - loss: 0.1075 - val_loss: 0.2453\n",
      "Epoch 19/80\n",
      " - 34s - loss: 0.1021 - val_loss: 0.2499\n",
      "Epoch 20/80\n",
      " - 34s - loss: 0.1006 - val_loss: 0.2480\n",
      "Epoch 21/80\n",
      " - 34s - loss: 0.0961 - val_loss: 0.2507\n",
      "Epoch 22/80\n",
      " - 34s - loss: 0.0911 - val_loss: 0.2558\n",
      "Epoch 23/80\n",
      " - 34s - loss: 0.0871 - val_loss: 0.2473\n",
      "Epoch 24/80\n",
      " - 34s - loss: 0.0843 - val_loss: 0.2529\n",
      "Epoch 25/80\n",
      " - 34s - loss: 0.0834 - val_loss: 0.2578\n",
      "Epoch 26/80\n",
      " - 34s - loss: 0.0802 - val_loss: 0.2530\n",
      "Epoch 27/80\n",
      " - 34s - loss: 0.0770 - val_loss: 0.2534\n",
      "Epoch 28/80\n",
      " - 34s - loss: 0.0740 - val_loss: 0.2499\n",
      "Epoch 29/80\n",
      " - 34s - loss: 0.0728 - val_loss: 0.2544\n",
      "Epoch 30/80\n",
      " - 34s - loss: 0.0687 - val_loss: 0.2617\n",
      "Epoch 31/80\n",
      " - 34s - loss: 0.0669 - val_loss: 0.2571\n",
      "Epoch 32/80\n",
      " - 34s - loss: 0.0639 - val_loss: 0.2583\n",
      "Epoch 33/80\n",
      " - 34s - loss: 0.0623 - val_loss: 0.2665\n",
      "Epoch 34/80\n",
      " - 34s - loss: 0.0605 - val_loss: 0.2676\n",
      "Epoch 35/80\n",
      " - 34s - loss: 0.0591 - val_loss: 0.2579\n",
      "Epoch 36/80\n",
      " - 34s - loss: 0.0564 - val_loss: 0.2632\n",
      "Epoch 37/80\n",
      " - 34s - loss: 0.0578 - val_loss: 0.2676\n",
      "Epoch 38/80\n",
      " - 34s - loss: 0.0565 - val_loss: 0.2637\n",
      "Epoch 39/80\n",
      " - 34s - loss: 0.0540 - val_loss: 0.2684\n",
      "Epoch 40/80\n",
      " - 34s - loss: 0.0526 - val_loss: 0.2664\n",
      "Epoch 41/80\n",
      " - 34s - loss: 0.0510 - val_loss: 0.2746\n",
      "Epoch 42/80\n",
      " - 34s - loss: 0.0502 - val_loss: 0.2745\n",
      "Epoch 43/80\n",
      " - 34s - loss: 0.0485 - val_loss: 0.2769\n",
      "Epoch 44/80\n",
      " - 34s - loss: 0.0471 - val_loss: 0.2739\n",
      "Epoch 45/80\n",
      " - 34s - loss: 0.0461 - val_loss: 0.2793\n",
      "Epoch 46/80\n",
      " - 34s - loss: 0.0447 - val_loss: 0.2779\n",
      "Epoch 47/80\n",
      " - 34s - loss: 0.0435 - val_loss: 0.2733\n",
      "Epoch 48/80\n",
      " - 34s - loss: 0.0422 - val_loss: 0.2749\n",
      "Epoch 49/80\n",
      " - 34s - loss: 0.0410 - val_loss: 0.2746\n",
      "Epoch 50/80\n",
      " - 34s - loss: 0.0401 - val_loss: 0.2778\n",
      "Epoch 51/80\n",
      " - 34s - loss: 0.0394 - val_loss: 0.2783\n",
      "Epoch 52/80\n",
      " - 34s - loss: 0.0385 - val_loss: 0.2854\n",
      "Epoch 53/80\n",
      " - 34s - loss: 0.0374 - val_loss: 0.2847\n",
      "Epoch 54/80\n",
      " - 34s - loss: 0.0364 - val_loss: 0.2913\n",
      "Epoch 55/80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "# 画像変換arrayの読み込み\n",
    "    train_1 = np.load('/root/userspace/data/data/vgg_array1.npy')\n",
    "    train_2 = np.load('/root/userspace/data/data/vgg_array2.npy')\n",
    "    train_3 = np.load('/root/userspace/data/data/vgg_array3.npy')\n",
    "    train_4 = np.load('/root/userspace/data/data/vgg_array4.npy')\n",
    "    train_X = np.r_[train_1, train_2, train_3, train_4]\n",
    "    valid_X = np.load('/root/userspace/data/data/vgg_valid.npy')\n",
    "    test_X = np.load('/root/userspace/data/data/vgg_test.npy')\n",
    "\n",
    "# caption読み込み\n",
    "    whole_texts1 = []\n",
    "    test_data = open(\"/root/userspace/data/text/caprion_all.txt\", \"r\")\n",
    "    for line in test_data:\n",
    "        whole_texts1.append(line.rstrip(\"\\n\"))\n",
    "    caption_test = []\n",
    "    test_data = open(\"/root/userspace/data/text/caprion_test.txt\", \"r\")\n",
    "    for line in test_data:\n",
    "        caption_test.append(line)\n",
    "    caption_valid = []\n",
    "    test_data = open(\"/root/userspace/data/text/caprion_valid.txt\", \"r\")\n",
    "    for line in test_data:\n",
    "        caption_valid.append(line)\n",
    "    caption_train = []\n",
    "    test_data = open(\"/root/userspace/data/text/caprion_train.txt\", \"r\")\n",
    "    for line in test_data:\n",
    "        caption_train.append(line)\n",
    "\n",
    "    for i in range(len(whole_texts1)):\n",
    "        whole_texts1[i] = \"<s> \" + whole_texts1[i].strip() + \" </s>\"\n",
    "\n",
    "    tokenizer_iu = Tokenizer(filters=\"\")\n",
    "    tokenizer_iu.fit_on_texts(whole_texts1)\n",
    "\n",
    "    for i in range(len(caption_test)):\n",
    "        caption_test[i] = \"<s> \" + caption_test[i].strip() + \" </s>\"\n",
    "    test_Y = tokenizer_iu.texts_to_sequences(caption_test)\n",
    "    test_Y = pad_sequences(test_Y, padding='post')\n",
    "    pad2 = np.zeros((116, 12), dtype = int)\n",
    "    test_Y = np.c_[test_Y, pad2]\n",
    "\n",
    "    for i in range(len(caption_valid)):\n",
    "        caption_valid[i] = \"<s> \" + caption_valid[i].strip() + \" </s>\"\n",
    "    valid_Y = tokenizer_iu.texts_to_sequences(caption_valid)\n",
    "    valid_Y = pad_sequences(valid_Y, padding='post')\n",
    "    pad1 = np.zeros((283, 3), dtype = int)\n",
    "    valid_Y = np.c_[valid_Y, pad1]\n",
    "\n",
    "    for i in range(len(caption_train)):\n",
    "        caption_train[i] = \"<s> \" + caption_train[i].strip() + \" </s>\"\n",
    "    train_Y = tokenizer_iu.texts_to_sequences(caption_train)\n",
    "    train_Y = pad_sequences(train_Y, padding='post')\n",
    "    train_Y = np.r_[train_Y, train_Y, train_Y, train_Y]\n",
    "\n",
    "    return train_X, valid_X, test_X, train_Y, valid_Y, test_Y, tokenizer_iu\n",
    "\n",
    "def compute_bleu(refs, preds):\n",
    "    return np.mean([sentence_bleu(r, p, emulate_multibleu=True) for r, p in zip(refs, preds)])\n",
    "\n",
    "def score_homework():\n",
    "    train_X, valid_X, test_X, train_Y, valid_Y, test_Y, tokenizer_iu = load_dataset()\n",
    "    decode_sequence = homework(train_X, valid_X, train_Y, valid_Y, tokenizer_iu)\n",
    "\n",
    "    bos_eos = tokenizer_iu.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "    output = [decode_sequence(test_X[i][np.newaxis,:], bos_eos, 100) for i in range(len(test_X))]\n",
    "    \n",
    "    detokenizer_iu = dict(map(reversed, tokenizer_iu.word_index.items()))\n",
    "    \n",
    "    preds = [[detokenizer_iu[i] for i in output[n][1:-1]] for n in range(len(output))]\n",
    "    refs = [[detokenizer_iu[i] for i in test_Y[n][1:-(np.count_nonzero(test_Y[n]==0)+1)]] for n in range(len(test_Y))]\n",
    "    refs = [[seq] for seq in refs]\n",
    "    \n",
    "    print(compute_bleu(refs, preds))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    score_homework()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_iu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0dd9b4ff7605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetokenizer_iu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_iu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_iu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_iu' is not defined"
     ]
    }
   ],
   "source": [
    "detokenizer_iu = dict(map(reversed, tokenizer_iu.word_index.items()))\n",
    "\n",
    "text_no = 0\n",
    "input_seq = np.reshape(test_X[text_no], (1, 9, 9, 512))\n",
    "bos_eos = tokenizer_iu.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "\n",
    "print('生成文:', ' '.join([detokenizer_iu[i] for i in decode_sequence(input_seq, bos_eos)]))\n",
    "test_Y_array = np.array(test_Y[text_no])[np.nonzero(np.array(test_Y[text_no]))]\n",
    "test_Y_list = list(test_y_array)\n",
    "print('正解文:', ' '.join([detokenizer_iu[i] for i in test_y_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_iu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-306df4ceaab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetokenizer_iu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_iu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbos_eos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_iu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_iu' is not defined"
     ]
    }
   ],
   "source": [
    "detokenizer_iu = dict(map(reversed, tokenizer_iu.word_index.items()))\n",
    "\n",
    "text_no = 0\n",
    "input_seq = np.reshape(test_X[text_no], (1, 8, 8, 2048))\n",
    "bos_eos = tokenizer_iu.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "\n",
    "print('生成文:', ' '.join([detokenizer_iu[i] for i in decode_sequence(input_seq, bos_eos)]))\n",
    "test_Y_array = np.array(test_Y[text_no])[np.nonzero(np.array(test_Y[text_no]))]\n",
    "test_Y_list = list(test_Y_array)\n",
    "print('正解文:', ' '.join([detokenizer_iu[i] for i in test_Y_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
